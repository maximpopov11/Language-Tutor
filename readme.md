This project originally aimed to compare the performance of an LLM model in responding to a language-learning prompt with a pre-prompt vs without and with a
post-prompt vs without. Given that grading the responses was difficult, an LLM was used to grade the responses instead of a human. As such, the project aim
deviated to primarily assessing the quality of said grading process.

In order to run the code, in main.py there is a main function that can be used to gather data and process it. For gathering data, you need to have an anthropic
API key. To process the data you do not need anything beyond the already gathered data.
